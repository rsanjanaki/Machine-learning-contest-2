{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcDqbgvPY_5M",
        "outputId": "2675350c-3ad2-4485-cfc5-9fe737d47556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (1200, 3074) Test: (1200, 3073)\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting IDs, Labels, and Pixel Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M5fn3u0BY_5P"
      },
      "outputs": [],
      "source": [
        "train_labels = train_df[\"y\"].values.astype(np.int64) - 1 \n",
        "train_df = train_df.drop(columns=[\"id\", \"y\"])\n",
        "\n",
        "test_ids = test_df[\"id\"].values\n",
        "test_df = test_df.drop(columns=[\"id\"])\n",
        "\n",
        "train_pixels = train_df.values.astype(np.float32)\n",
        "test_pixels  = test_df.values .astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reshaping\n",
        "\n",
        "image_array function, reshapes each flat 3072-vector into a 3×32×32 image tensor (channels_first format)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4-em6A4kY_5Q"
      },
      "outputs": [],
      "source": [
        "def image_array(flat_array):\n",
        "    return flat_array.reshape(-1, 3, 32, 32)\n",
        "\n",
        "X_train = image_array(train_pixels)\n",
        "X_test  = image_array(test_pixels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Split to Training and Validation\n",
        "\n",
        "Stratified split into 80% train / 20% validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, train_labels, test_size=0.2, stratify=train_labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building a PyTorch Dataset and DataLoaders\n",
        "\n",
        "It wraps NumPy arrays in a PyTorch Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j0MInKnaY_5R"
      },
      "outputs": [],
      "source": [
        "class FarmImageDataset(Dataset):\n",
        "    def __init__(self, images, labels=None):\n",
        "        self.images = torch.from_numpy(images)      # float32 tensor\n",
        "        self.labels = None if labels is None else torch.from_numpy(labels)\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.images[idx]\n",
        "        if self.labels is None:\n",
        "            return x\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "train_ds = FarmImageDataset(X_tr, y_tr)\n",
        "val_ds   = FarmImageDataset(X_val, y_val)\n",
        "test_ds  = FarmImageDataset(X_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the CNN Model\n",
        "\n",
        "Two convolutional blocks:\n",
        "\n",
        "Conv2d(3→32), ReLU, MaxPool → outputs 32×16×16\n",
        "Conv2d(32→64), ReLU, MaxPool → outputs 64×8×8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "beIAy8N4Y_5S"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                             \n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),                                 \n",
        "            nn.Linear(64*8*8, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Validation Loops\n",
        "\n",
        "Using Adam optimizer with learning rate 1e-3 and standard cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One epoch of training\n",
        "\n",
        "Forward pass → compute loss → backward → optimizer step.\n",
        "\n",
        "Accumulates total loss and total correct predictions to return average loss & accuracy.\n",
        "\n",
        "Evaluation on validation (no gradient updates)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/stock/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import optuna\n",
        "\n",
        "# from torch import nn, optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# class TunableCNN(nn.Module):\n",
        "#     def __init__(self, dropout_rate):\n",
        "#         super().__init__()\n",
        "#         self.conv_layers = nn.Sequential(\n",
        "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2),\n",
        "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2)\n",
        "#         )\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(64 * 8 * 8, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(128, 3)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.classifier(self.conv_layers(x))\n",
        "\n",
        "\n",
        "# def train_epoch(model, loader, criterion, optimizer):\n",
        "#     model.train()\n",
        "#     total_loss, total_correct = 0, 0\n",
        "#     for Xb, yb in loader:\n",
        "#         Xb, yb = Xb.to(device), yb.to(device)\n",
        "#         preds  = model(Xb)\n",
        "#         loss   = criterion(preds, yb)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss    += loss.item() * Xb.size(0)\n",
        "#         total_correct += (preds.argmax(1) == yb).sum().item()\n",
        "#     return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "# def eval_epoch(model, loader, criterion):\n",
        "#     model.eval()\n",
        "#     total_loss, total_correct = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for Xb, yb in loader:\n",
        "#             Xb, yb = Xb.to(device), yb.to(device)\n",
        "#             preds  = model(Xb)\n",
        "#             total_loss    += criterion(preds, yb).item() * Xb.size(0)\n",
        "#             total_correct += (preds.argmax(1) == yb).sum().item()\n",
        "#     return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def objective(trial):\n",
        "#     lr           = trial.suggest_loguniform('lr',    1e-5, 1e-2)\n",
        "#     momentum     = trial.suggest_float('momentum',  0.0, 0.99)\n",
        "#     dropout_rate = trial.suggest_float('dropout',   0.2, 0.7, step=0.1)\n",
        "#     batch_size   = trial.suggest_categorical('batch_size',[32,64,128])\n",
        "#     n_epochs     = trial.suggest_int('epochs', 10, 50, step=5)\n",
        "    \n",
        "#     model     = TunableCNN(dropout_rate).to(device)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "#     train_loader = DataLoader(FarmImageDataset(X_tr, y_tr),\n",
        "#                               batch_size=batch_size, shuffle=True)\n",
        "#     val_loader   = DataLoader(FarmImageDataset(X_val, y_val),\n",
        "#                               batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     best_val_acc = 0.0\n",
        "#     for epoch in range(1, n_epochs + 1):\n",
        "#         train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "#         val_loss, val_acc     = eval_epoch(model, val_loader,   criterion)\n",
        "\n",
        "\n",
        "#         trial.report(val_acc, epoch)\n",
        "#         if trial.should_prune():\n",
        "#             raise optuna.TrialPruned()\n",
        "\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "\n",
        "#     return best_val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-06 23:18:09,314] A new study created in memory with name: no-name-140066df-c182-4b88-829e-71bf97c67fa8\n",
            "/var/folders/tq/ht38rm5x6db5s9_5q7fx5xqc0000gn/T/ipykernel_37948/3293525918.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr           = trial.suggest_loguniform('lr',    1e-5, 1e-2)\n",
            "[I 2025-05-06 23:18:25,996] Trial 0 finished with value: 0.8333333333333334 and parameters: {'lr': 0.001193584091184172, 'momentum': 0.8175326261230483, 'dropout': 0.30000000000000004, 'batch_size': 128, 'epochs': 50}. Best is trial 0 with value: 0.8333333333333334.\n",
            "[I 2025-05-06 23:18:40,856] Trial 1 finished with value: 0.375 and parameters: {'lr': 9.361512409633552e-05, 'momentum': 0.9094694864943689, 'dropout': 0.30000000000000004, 'batch_size': 128, 'epochs': 45}. Best is trial 0 with value: 0.8333333333333334.\n",
            "[I 2025-05-06 23:18:45,090] Trial 2 finished with value: 0.3875 and parameters: {'lr': 0.00013197597580010998, 'momentum': 0.06969746923246922, 'dropout': 0.2, 'batch_size': 32, 'epochs': 10}. Best is trial 0 with value: 0.8333333333333334.\n",
            "[I 2025-05-06 23:18:51,680] Trial 3 finished with value: 0.36666666666666664 and parameters: {'lr': 0.0005465858139686958, 'momentum': 0.78092271067732, 'dropout': 0.7, 'batch_size': 128, 'epochs': 20}. Best is trial 0 with value: 0.8333333333333334.\n",
            "[I 2025-05-06 23:19:00,142] Trial 4 finished with value: 0.4041666666666667 and parameters: {'lr': 0.0004280706544215559, 'momentum': 0.6523012720274973, 'dropout': 0.2, 'batch_size': 32, 'epochs': 20}. Best is trial 0 with value: 0.8333333333333334.\n",
            "[I 2025-05-06 23:19:00,506] Trial 5 pruned. \n",
            "[I 2025-05-06 23:19:00,842] Trial 6 pruned. \n",
            "[I 2025-05-06 23:19:01,264] Trial 7 pruned. \n",
            "[I 2025-05-06 23:19:01,695] Trial 8 pruned. \n",
            "[I 2025-05-06 23:19:02,119] Trial 9 pruned. \n",
            "[I 2025-05-06 23:19:20,103] Trial 10 finished with value: 0.9541666666666667 and parameters: {'lr': 0.0035722662860218075, 'momentum': 0.9713655185904604, 'dropout': 0.5, 'batch_size': 64, 'epochs': 50}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:19:20,473] Trial 11 pruned. \n",
            "[I 2025-05-06 23:19:20,840] Trial 12 pruned. \n",
            "[I 2025-05-06 23:19:21,209] Trial 13 pruned. \n",
            "[I 2025-05-06 23:19:21,549] Trial 14 pruned. \n",
            "[I 2025-05-06 23:19:35,838] Trial 15 finished with value: 0.9416666666666667 and parameters: {'lr': 0.0013407365480967558, 'momentum': 0.9810342026588347, 'dropout': 0.5, 'batch_size': 64, 'epochs': 40}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:19:50,176] Trial 16 finished with value: 0.9541666666666667 and parameters: {'lr': 0.00959287356089014, 'momentum': 0.9790098119270461, 'dropout': 0.6000000000000001, 'batch_size': 64, 'epochs': 40}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:19:58,076] Trial 17 pruned. \n",
            "[I 2025-05-06 23:19:58,448] Trial 18 pruned. \n",
            "[I 2025-05-06 23:19:58,818] Trial 19 pruned. \n",
            "[I 2025-05-06 23:19:59,187] Trial 20 pruned. \n",
            "[I 2025-05-06 23:20:07,112] Trial 21 pruned. \n",
            "[I 2025-05-06 23:20:07,483] Trial 22 pruned. \n",
            "[I 2025-05-06 23:20:14,965] Trial 23 pruned. \n",
            "[I 2025-05-06 23:20:15,339] Trial 24 pruned. \n",
            "[I 2025-05-06 23:20:15,711] Trial 25 pruned. \n",
            "[I 2025-05-06 23:20:16,083] Trial 26 pruned. \n",
            "[I 2025-05-06 23:20:16,451] Trial 27 pruned. \n",
            "[I 2025-05-06 23:20:16,823] Trial 28 pruned. \n",
            "[I 2025-05-06 23:20:17,190] Trial 29 pruned. \n",
            "[I 2025-05-06 23:20:24,178] Trial 30 pruned. \n",
            "[I 2025-05-06 23:20:24,523] Trial 31 pruned. \n",
            "[I 2025-05-06 23:20:24,869] Trial 32 pruned. \n",
            "[I 2025-05-06 23:20:25,212] Trial 33 pruned. \n",
            "[I 2025-05-06 23:20:25,554] Trial 34 pruned. \n",
            "[I 2025-05-06 23:20:25,896] Trial 35 pruned. \n",
            "[I 2025-05-06 23:20:26,329] Trial 36 pruned. \n",
            "[I 2025-05-06 23:20:33,302] Trial 37 pruned. \n",
            "[I 2025-05-06 23:20:33,741] Trial 38 pruned. \n",
            "[I 2025-05-06 23:20:34,118] Trial 39 pruned. \n",
            "[I 2025-05-06 23:20:34,459] Trial 40 pruned. \n",
            "[I 2025-05-06 23:20:40,853] Trial 41 finished with value: 0.8541666666666666 and parameters: {'lr': 0.0004803188608255427, 'momentum': 0.9391520511443858, 'dropout': 0.2, 'batch_size': 32, 'epochs': 15}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:20:41,290] Trial 42 pruned. \n",
            "[I 2025-05-06 23:20:41,732] Trial 43 pruned. \n",
            "[I 2025-05-06 23:20:42,170] Trial 44 pruned. \n",
            "[I 2025-05-06 23:20:52,919] Trial 45 finished with value: 0.9458333333333333 and parameters: {'lr': 0.0012834925570023522, 'momentum': 0.9878270066812485, 'dropout': 0.4, 'batch_size': 32, 'epochs': 25}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:20:53,358] Trial 46 pruned. \n",
            "[I 2025-05-06 23:20:59,746] Trial 47 finished with value: 0.9083333333333333 and parameters: {'lr': 0.0064441104754512065, 'momentum': 0.9379488954273915, 'dropout': 0.5, 'batch_size': 32, 'epochs': 15}. Best is trial 10 with value: 0.9541666666666667.\n",
            "[I 2025-05-06 23:21:00,192] Trial 48 pruned. \n",
            "[I 2025-05-06 23:21:00,633] Trial 49 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'lr': 0.0035722662860218075, 'momentum': 0.9713655185904604, 'dropout': 0.5, 'batch_size': 64, 'epochs': 50}\n",
            "Best validation accuracy: 0.9541666666666667\n"
          ]
        }
      ],
      "source": [
        "# study = optuna.create_study(direction='maximize',\n",
        "#                             pruner=optuna.pruners.MedianPruner())\n",
        "# study.optimize(objective, n_trials=50)\n",
        "\n",
        "# print(\"Best hyperparameters:\", study.best_params)\n",
        "# print(\"Best validation accuracy:\", study.best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 – train_acc: 0.369, val_acc: 0.338\n",
            "Epoch 02 – train_acc: 0.418, val_acc: 0.512\n",
            "Epoch 03 – train_acc: 0.539, val_acc: 0.617\n",
            "Epoch 04 – train_acc: 0.665, val_acc: 0.725\n",
            "Epoch 05 – train_acc: 0.733, val_acc: 0.754\n",
            "Epoch 06 – train_acc: 0.792, val_acc: 0.825\n",
            "Epoch 07 – train_acc: 0.767, val_acc: 0.762\n",
            "Epoch 08 – train_acc: 0.789, val_acc: 0.688\n",
            "Epoch 09 – train_acc: 0.756, val_acc: 0.758\n",
            "Epoch 10 – train_acc: 0.797, val_acc: 0.758\n",
            "Epoch 11 – train_acc: 0.847, val_acc: 0.792\n",
            "Epoch 12 – train_acc: 0.852, val_acc: 0.875\n",
            "Epoch 13 – train_acc: 0.866, val_acc: 0.863\n",
            "Epoch 14 – train_acc: 0.889, val_acc: 0.879\n",
            "Epoch 15 – train_acc: 0.907, val_acc: 0.904\n",
            "Epoch 16 – train_acc: 0.911, val_acc: 0.904\n",
            "Epoch 17 – train_acc: 0.918, val_acc: 0.904\n",
            "Epoch 18 – train_acc: 0.944, val_acc: 0.883\n",
            "Epoch 19 – train_acc: 0.926, val_acc: 0.904\n",
            "Epoch 20 – train_acc: 0.915, val_acc: 0.892\n",
            "Epoch 21 – train_acc: 0.910, val_acc: 0.900\n",
            "Epoch 22 – train_acc: 0.924, val_acc: 0.921\n",
            "Epoch 23 – train_acc: 0.929, val_acc: 0.929\n",
            "Epoch 24 – train_acc: 0.947, val_acc: 0.921\n",
            "Epoch 25 – train_acc: 0.944, val_acc: 0.883\n",
            "Epoch 26 – train_acc: 0.938, val_acc: 0.933\n",
            "Epoch 27 – train_acc: 0.953, val_acc: 0.933\n",
            "Epoch 28 – train_acc: 0.961, val_acc: 0.942\n",
            "Epoch 29 – train_acc: 0.948, val_acc: 0.950\n",
            "Epoch 30 – train_acc: 0.956, val_acc: 0.933\n",
            "Epoch 31 – train_acc: 0.964, val_acc: 0.921\n",
            "Epoch 32 – train_acc: 0.936, val_acc: 0.950\n",
            "Epoch 33 – train_acc: 0.915, val_acc: 0.887\n",
            "Epoch 34 – train_acc: 0.915, val_acc: 0.933\n",
            "Epoch 35 – train_acc: 0.926, val_acc: 0.929\n",
            "Epoch 36 – train_acc: 0.958, val_acc: 0.921\n",
            "Epoch 37 – train_acc: 0.950, val_acc: 0.933\n",
            "Epoch 38 – train_acc: 0.951, val_acc: 0.933\n",
            "Epoch 39 – train_acc: 0.972, val_acc: 0.929\n",
            "Epoch 40 – train_acc: 0.971, val_acc: 0.950\n",
            "Epoch 41 – train_acc: 0.970, val_acc: 0.942\n",
            "Epoch 42 – train_acc: 0.967, val_acc: 0.954\n",
            "Epoch 43 – train_acc: 0.961, val_acc: 0.933\n",
            "Epoch 44 – train_acc: 0.946, val_acc: 0.950\n",
            "Epoch 45 – train_acc: 0.964, val_acc: 0.946\n",
            "Epoch 46 – train_acc: 0.975, val_acc: 0.942\n",
            "Epoch 47 – train_acc: 0.979, val_acc: 0.950\n",
            "Epoch 48 – train_acc: 0.977, val_acc: 0.954\n",
            "Epoch 49 – train_acc: 0.981, val_acc: 0.954\n",
            "Epoch 50 – train_acc: 0.978, val_acc: 0.946\n",
            "Best validation accuracy: 0.954\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# from torch import optim, nn\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# best_lr        = 0.0035722662860218075\n",
        "# best_momentum  = 0.9713655185904604\n",
        "# best_dropout   = 0.5   \n",
        "# best_batch     = 64\n",
        "# best_epochs    = 50\n",
        "\n",
        "# train_loader = DataLoader(train_ds, batch_size=best_batch, shuffle=True)\n",
        "# val_loader   = DataLoader(val_ds,   batch_size=best_batch)\n",
        "\n",
        "# model     = SimpleCNN().to(device)   \n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(\n",
        "#     model.parameters(),\n",
        "#     lr=best_lr,\n",
        "#     momentum=best_momentum\n",
        "# )\n",
        "\n",
        "# best_val_acc = 0.0\n",
        "# for epoch in range(1, best_epochs + 1):\n",
        "   \n",
        "#     model.train()\n",
        "#     total_loss, total_correct = 0.0, 0\n",
        "#     for Xb, yb in train_loader:\n",
        "#         Xb, yb = Xb.to(device), yb.to(device)\n",
        "#         preds  = model(Xb)\n",
        "#         loss   = criterion(preds, yb)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss    += loss.item() * Xb.size(0)\n",
        "#         total_correct += (preds.argmax(1) == yb).sum().item()\n",
        "#     train_acc = total_correct / len(train_loader.dataset)\n",
        "\n",
        "  \n",
        "#     model.eval()\n",
        "#     val_loss, val_correct = 0.0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for Xb, yb in val_loader:\n",
        "#             Xb, yb = Xb.to(device), yb.to(device)\n",
        "#             preds  = model(Xb)\n",
        "#             val_loss    += criterion(preds, yb).item() * Xb.size(0)\n",
        "#             val_correct += (preds.argmax(1) == yb).sum().item()\n",
        "#     val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "#     if val_acc > best_val_acc:\n",
        "#         best_val_acc = val_acc\n",
        "#         torch.save(model.state_dict(), \"cnn_model_tuned.pth\")\n",
        "\n",
        "#     print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n",
        "\n",
        "# print(f\"Best validation accuracy: {best_val_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9GWkO75Y_5T",
        "outputId": "33d3fb82-d49e-46bc-c728-b449935eadf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 – train_acc: 0.600, val_acc: 0.812\n",
            "Epoch 02 – train_acc: 0.803, val_acc: 0.846\n",
            "Epoch 03 – train_acc: 0.847, val_acc: 0.858\n",
            "Epoch 04 – train_acc: 0.882, val_acc: 0.879\n",
            "Epoch 05 – train_acc: 0.898, val_acc: 0.867\n",
            "Epoch 06 – train_acc: 0.887, val_acc: 0.904\n",
            "Epoch 07 – train_acc: 0.904, val_acc: 0.904\n",
            "Epoch 08 – train_acc: 0.931, val_acc: 0.921\n",
            "Epoch 09 – train_acc: 0.927, val_acc: 0.908\n",
            "Epoch 10 – train_acc: 0.936, val_acc: 0.938\n",
            "Epoch 11 – train_acc: 0.944, val_acc: 0.950\n",
            "Epoch 12 – train_acc: 0.954, val_acc: 0.942\n",
            "Epoch 13 – train_acc: 0.952, val_acc: 0.946\n",
            "Epoch 14 – train_acc: 0.947, val_acc: 0.963\n",
            "Epoch 15 – train_acc: 0.951, val_acc: 0.908\n",
            "Epoch 16 – train_acc: 0.942, val_acc: 0.933\n",
            "Epoch 17 – train_acc: 0.961, val_acc: 0.967\n",
            "Epoch 18 – train_acc: 0.968, val_acc: 0.950\n",
            "Epoch 19 – train_acc: 0.950, val_acc: 0.942\n",
            "Epoch 20 – train_acc: 0.964, val_acc: 0.912\n",
            "Epoch 21 – train_acc: 0.966, val_acc: 0.963\n",
            "Epoch 22 – train_acc: 0.963, val_acc: 0.933\n",
            "Epoch 23 – train_acc: 0.965, val_acc: 0.967\n",
            "Epoch 24 – train_acc: 0.972, val_acc: 0.963\n",
            "Epoch 25 – train_acc: 0.970, val_acc: 0.967\n",
            "Epoch 26 – train_acc: 0.974, val_acc: 0.963\n",
            "Epoch 27 – train_acc: 0.968, val_acc: 0.946\n",
            "Epoch 28 – train_acc: 0.966, val_acc: 0.950\n",
            "Epoch 29 – train_acc: 0.983, val_acc: 0.963\n",
            "Epoch 30 – train_acc: 0.989, val_acc: 0.971\n",
            "Epoch 31 – train_acc: 0.984, val_acc: 0.954\n",
            "Epoch 32 – train_acc: 0.988, val_acc: 0.954\n",
            "Epoch 33 – train_acc: 0.991, val_acc: 0.967\n",
            "Epoch 34 – train_acc: 0.991, val_acc: 0.963\n",
            "Epoch 35 – train_acc: 0.991, val_acc: 0.963\n",
            "Epoch 36 – train_acc: 0.994, val_acc: 0.963\n",
            "Epoch 37 – train_acc: 0.990, val_acc: 0.963\n",
            "Epoch 38 – train_acc: 0.984, val_acc: 0.963\n",
            "Epoch 39 – train_acc: 0.990, val_acc: 0.963\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "        total_correct += (preds.argmax(1) == yb).sum().item()\n",
        "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
        "\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            preds = model(Xb)\n",
        "            total_loss += criterion(preds, yb).item() * Xb.size(0)\n",
        "            total_correct += (preds.argmax(1) == yb).sum().item()\n",
        "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
        "\n",
        "best_val_acc = 0\n",
        "for epoch in range(1, 40):\n",
        "    train_loss, train_acc = train_epoch(train_loader)\n",
        "    val_loss, val_acc     = eval_epoch(val_loader)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"cnn_model.pth\")\n",
        "    print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Measuring the Final Model's Performance on the Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy16ZbGiY_5V",
        "outputId": "207112fc-3ece-4e60-c25c-47b675b9e227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final validation accuracy (re-loaded best model): 97.08%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"cnn_model.pth\"))\n",
        "val_loss, val_acc = eval_epoch(val_loader)\n",
        "print(f\"Final validation accuracy (re-loaded best model): {val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retraining the Final Model on the Combination of Train and Val Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Full train] Epoch 01 – train_acc: 0.657\n",
            "[Full train] Epoch 02 – train_acc: 0.813\n",
            "[Full train] Epoch 03 – train_acc: 0.835\n",
            "[Full train] Epoch 04 – train_acc: 0.880\n",
            "[Full train] Epoch 05 – train_acc: 0.906\n",
            "[Full train] Epoch 06 – train_acc: 0.905\n",
            "[Full train] Epoch 07 – train_acc: 0.922\n",
            "[Full train] Epoch 08 – train_acc: 0.933\n",
            "[Full train] Epoch 09 – train_acc: 0.951\n",
            "[Full train] Epoch 10 – train_acc: 0.938\n",
            "[Full train] Epoch 11 – train_acc: 0.950\n",
            "[Full train] Epoch 12 – train_acc: 0.958\n",
            "[Full train] Epoch 13 – train_acc: 0.971\n",
            "[Full train] Epoch 14 – train_acc: 0.965\n",
            "[Full train] Epoch 15 – train_acc: 0.963\n",
            "[Full train] Epoch 16 – train_acc: 0.963\n",
            "[Full train] Epoch 17 – train_acc: 0.963\n",
            "[Full train] Epoch 18 – train_acc: 0.927\n",
            "[Full train] Epoch 19 – train_acc: 0.958\n",
            "[Full train] Epoch 20 – train_acc: 0.973\n",
            "[Full train] Epoch 21 – train_acc: 0.973\n",
            "[Full train] Epoch 22 – train_acc: 0.983\n",
            "[Full train] Epoch 23 – train_acc: 0.975\n",
            "[Full train] Epoch 24 – train_acc: 0.981\n",
            "[Full train] Epoch 25 – train_acc: 0.987\n",
            "[Full train] Epoch 26 – train_acc: 0.991\n",
            "[Full train] Epoch 27 – train_acc: 0.983\n",
            "[Full train] Epoch 28 – train_acc: 0.991\n",
            "[Full train] Epoch 29 – train_acc: 0.996\n",
            "[Full train] Epoch 30 – train_acc: 0.991\n",
            "[Full train] Epoch 31 – train_acc: 0.992\n",
            "[Full train] Epoch 32 – train_acc: 0.992\n",
            "[Full train] Epoch 33 – train_acc: 0.994\n",
            "[Full train] Epoch 34 – train_acc: 0.990\n",
            "[Full train] Epoch 35 – train_acc: 0.973\n",
            "[Full train] Epoch 36 – train_acc: 0.990\n",
            "[Full train] Epoch 37 – train_acc: 0.996\n",
            "[Full train] Epoch 38 – train_acc: 0.995\n",
            "[Full train] Epoch 39 – train_acc: 0.997\n"
          ]
        }
      ],
      "source": [
        "X_full = np.concatenate([X_tr, X_val], axis=0)\n",
        "y_full = np.concatenate([y_tr,   y_val  ], axis=0)\n",
        "\n",
        "full_ds     = FarmImageDataset(X_full, y_full)\n",
        "full_loader = DataLoader(full_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "model     = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, 40):\n",
        "    train_loss, train_acc = train_epoch(full_loader)\n",
        "    print(f\"[Full train] Epoch {epoch:02d} – train_acc: {train_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feeding the Test Set to the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cnn_submission_8.csv with 1200 rows.\n"
          ]
        }
      ],
      "source": [
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for Xb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        preds = model(Xb).argmax(1).cpu().numpy() + 1 \n",
        "        all_preds.append(preds)\n",
        "all_preds = np.concatenate(all_preds)\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test_ids, \"y\": all_preds})\n",
        "submission.to_csv(\"cnn_submission_8.csv\", index=False)\n",
        "print(\"cnn_submission_8.csv with\", len(submission), \"rows.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stock",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
